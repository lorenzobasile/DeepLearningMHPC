{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inkmEpop3Rb7"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorenzobasile/DeepLearningMHPC/blob/main/3_adv_attention.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3YBQpy13Rb8"
   },
   "source": [
    "# Lab 3: Adversarial attacks and Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap from previous Lab\n",
    "\n",
    "* We saw the main techniques to mitigate overfitting in neural networks;\n",
    "* We built and trained a convolutional network for image classification;\n",
    "* We saw how to leverage pre-trained parameters to transfer the network's knowledge to new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Today\n",
    "\n",
    "We will show an example of one of the most intriguing phenomena in neural networks, **adversarial vulnerability**, in the context of CIFAR-10 classification. Then, we will move away from CNNs and towards transformers, by implementing the **attention** layer and a simple transformer model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial attacks are small and maliciously crafted perturbations to the input data of DL systems, usually imperceptible to human eyes but highly disruptive for the neural network. By exploiting the modelâ€™s sensitivity to slight changes in input, these attacks can cause incorrect and often unpredictable behavior, even in well-trained systems. \n",
    "\n",
    "<img src=\"./images/adv.png\" width=\"800\"/>\n",
    "\n",
    "This phenomenon reveals fundamental weaknesses in how neural networks process information, highlighting a gap between human perception and machine learning models. Understanding adversarial attacks is crucial for assessing the reliability of deep learning, especially in applications where robustness and security are critical, such as autonomous driving or medical diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a CNN on CIFAR-10\n",
    "\n",
    "The first step we take is training a convolutional classifier on CIFAR-10 images. CIFAR-10 is another popular benchmark dataset, much like MNIST. However, it represents a significant step up in terms of complexity from MNIST: images are now coloured, and slightly larger (each image is represented as 32x32 pixels over 3 (red, green, blue) channels). They belong to 10 categories: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
    "\n",
    "We are not aiming for state-of-the-art performance, but to improve the classification capability of our CNN, we can use light data augmentation. When using `torchvision` datasets, it is very simple to include data augmentation by exploiting the `transforms` submodule. Each time an image is loaded from the dataloader, it may get horizontally flipped and/or cropped, adding to the variability of the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomCrop(32, 4),\n",
    "        torchvision.transforms.ToTensor()\n",
    "        ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ]),\n",
    "}\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms['train'])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms['test'])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data and network architecture we will be using today start to approach real-world sizes. For this experiment, it is a good idea to speed-up training through GPU acceleration. GPU runtime can be chosen in Colab by simply using the menu option `Runtime>Change runtime type`. Once this has been done, the following PyTorch command will automatically perceive `cuda:0` (the first GPU) as the current device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network has a simple structure, with 2 convolutional layers and ReLU activations. We have to pay extra attention to the device: by default, PyTorch loads the model (and everything else) to CPU; if a GPU has to be used, the model must be loaded to the correct device by using `.to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, stride=1, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.pool = torch.nn.Sequential(\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2),\n",
    "            torch.nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.head = torch.nn.Linear(128*7*7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        self.last_feature_map = x\n",
    "        x = self.pool(x)\n",
    "        return self.head(x)\n",
    "\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just the usual optimizer and loss definition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the usual code to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct=0\n",
    "        for x, y in iter(dataloader):\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            out=model(x)\n",
    "            correct+=(torch.argmax(out, axis=1)==y).sum()\n",
    "        return (correct/len(dataloader.dataset)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model reaches a competitive accuracy relatively quickly. This value is far from the state-of-the-art on CIFAR-10 (which is well above 99%), but it just serves as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "for epoch in range(epochs):\n",
    "    print(\"Test accuracy: \", get_accuracy(model, testloader, device))\n",
    "    model.train()\n",
    "    print(\"Epoch: \", epoch)\n",
    "    for x, y in iter(trainloader):\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        out=model(x)\n",
    "        l=loss(out, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "print(\"Final accuracy: \", get_accuracy(model, testloader, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Gradient Sign attack\n",
    "\n",
    "We will implement the simplest form of gradient-based adversarial attack, the Fast Gradient Sign Method (FGSM), introduced by Goodfellow et al. in 2014. Given an input image $x$, the adversarial perturbation is simply computed as the gradient of the loss function (in our case, the cross-entropy) with respect to $x$:\n",
    "$$\n",
    "\\Delta = \\text{sign}(\\nabla_x L(y,\\hat{y}))\n",
    "$$\n",
    "The perturbation is then rescaled by a factor $\\epsilon$, so that it does not exceed a given threshold in $\\ell_\\infty$ norm. Common values for $\\epsilon$ include $\\{1,2,3,...,8\\}/255$.\n",
    "\n",
    "The adversarial image is then obtained as:\n",
    "$$\n",
    "x'=x+\\epsilon\\Delta\n",
    "$$\n",
    "After this computation, we clamp the perturbed image in $[0,1]$ as this was the original range for the pixels of the clean image.\n",
    "\n",
    "In this lab, given its simplicity, we are implementing FGSM from scratch. However, in standard research practise, attack algorithms can be taken directly from libraries, such as [advertorch](https://advertorch.readthedocs.io/en/latest/index.html) or [foolbox](https://foolbox.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, image, label, epsilon=8/255):\n",
    "    # Ensure the image requires gradients for the attack\n",
    "    image.requires_grad = True\n",
    "\n",
    "    # Forward pass: Get the model's prediction\n",
    "    output = model(image)\n",
    "    loss = torch.nn.functional.cross_entropy(output, label)\n",
    "\n",
    "    # Backward pass: Compute gradients of the loss w.r.t. the image\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Get the sign of the gradients\n",
    "    sign_data_grad = image.grad.sign()\n",
    "\n",
    "    # Create adversarial example\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)  # Keep pixel values valid\n",
    "\n",
    "\n",
    "    return perturbed_image, perturbed_image-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to compute gradients to apply FGSM, hence the context manager `torch.no_grad()` cannot be applied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adversarial_accuracy(model, dataloader, attack, device):\n",
    "    model.eval()\n",
    "    correct=0\n",
    "    for x, y in iter(dataloader):\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        adv, _ =attack(model, x, y)\n",
    "        with torch.no_grad():\n",
    "            out=model(adv)\n",
    "            correct+=(torch.argmax(out, axis=1)==y).sum()\n",
    "    return (correct/len(dataloader.dataset)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very small perturbation, $\\epsilon=\\frac{4}{255}$ is already enough to disrupt most of the classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "get_adversarial_accuracy(model, testloader, partial(fgsm_attack, epsilon=4/255), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize a few examples of clean and adversarially attacked images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(iter(testloader))\n",
    "x=x.to(device)\n",
    "y=y.to(device)\n",
    "adv, pert =fgsm_attack(model, x, y, 4/255)\n",
    "adversarial_y = model(adv).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_idx=42\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(x[image_idx].detach().cpu().permute(1, 2, 0))\n",
    "axes[0].set_title(f\"Clean: {testset.classes[y[image_idx]]}\")\n",
    "\n",
    "axes[1].imshow(adv[image_idx].detach().cpu().permute(1, 2, 0))\n",
    "axes[1].set_title(f\"Adversarial: {testset.classes[adversarial_y[image_idx]]}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial defense\n",
    "\n",
    "Defending against adversarial attacks is complex, and it is still an open problem in DL research. \n",
    "\n",
    "A possible approach is to apply adversarial training. This method foresees a double optimization: the network gets fine-tuned using both clean and adversarial data. Note: adversarial data depends on current network weights; when doing adversarial training, the attack algorithm has to be run again after each optimization step.\n",
    "\n",
    "Adversarial training can significantly improve robustness, at the cost of a (hopefully small) drop in clean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(epochs):\n",
    "    print(\"Initial test accuracy: \", get_accuracy(model, testloader, device))\n",
    "    print(\"Initial adversarial accuracy: \", get_adversarial_accuracy(model, testloader,partial(fgsm_attack, epsilon=4/255), device))\n",
    "    model.train()\n",
    "    print(\"Epoch: \", epoch)\n",
    "    for x, y in iter(trainloader):\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        clean_out=model(x)\n",
    "        l=loss(clean_out, y)\n",
    "        adv, pert =fgsm_attack(model, x, y, 4/255)\n",
    "        adv_out=model(adv)\n",
    "        l+=loss(adv_out, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "print(\"Final clean accuracy: \", get_accuracy(model, testloader, device))\n",
    "print(\"Final adversarial accuracy: \", get_adversarial_accuracy(model, testloader,partial(fgsm_attack, epsilon=4/255), device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.d = output_dim\n",
    "        self.q_proj = torch.nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.k_proj = torch.nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.v_proj = torch.nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.out_proj = torch.nn.Linear(output_dim, output_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        weights = torch.nn.functional.softmax(torch.einsum('btd,bTd->btT', q, k)/math.sqrt(self.d), dim=-1)\n",
    "        #weights = torch.nn.functional.softmax(torch.bmm(q, k.permute(0,2,1))/math.sqrt(self.d), dim=-1)\n",
    "        out = torch.bmm(weights, v)\n",
    "        return self.out_proj(out)\n",
    "attn = Attention(128, 128)\n",
    "x=torch.randn(8, 10, 128)\n",
    "myattn = attn(x)\n",
    "\n",
    "y = x.permute(1, 0, 2)\n",
    "mha2_out = torch.nn.functional.multi_head_attention_forward(\n",
    "    query=y,\n",
    "    key=y,\n",
    "    value=y,\n",
    "    embed_dim_to_check=128,\n",
    "    num_heads=1,\n",
    "    use_separate_proj_weight=True,\n",
    "    in_proj_weight=None,\n",
    "    in_proj_bias=None,\n",
    "    bias_k=None,\n",
    "    bias_v=None,\n",
    "    need_weights=False,\n",
    "    q_proj_weight=attn.q_proj.weight,\n",
    "    k_proj_weight=attn.k_proj.weight,\n",
    "    v_proj_weight=attn.v_proj.weight,\n",
    "    out_proj_bias=attn.out_proj.bias,\n",
    "    out_proj_weight=attn.out_proj.weight,\n",
    "    add_zero_attn=False,\n",
    "    dropout_p=0,\n",
    ")[0].permute(1, 0, 2)\n",
    "print(torch.allclose(mha2_out, myattn, atol=1e-6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.d = output_dim\n",
    "        self.h = num_heads\n",
    "        assert self.d % self.h == 0, \"Output dimension must be divisible by the number of heads\"\n",
    "        self.q_proj = torch.nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.k_proj = torch.nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.v_proj = torch.nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.out_proj = torch.nn.Linear(output_dim, output_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, input_dim = x.shape\n",
    "        H = self.h\n",
    "        D = self.d\n",
    "        q = self.q_proj(x).reshape(B, T, H, D//H)\n",
    "        k = self.k_proj(x).reshape(B, T, H, D//H)\n",
    "        v = self.v_proj(x).reshape(B, T, H, D//H)\n",
    "\n",
    "        weights = torch.einsum('bthd,bThd->bhtT', q, k)/math.sqrt(D//H)\n",
    "        weights = torch.nn.functional.softmax(weights, dim=-1)\n",
    "        out = torch.einsum('bhtT,bThd->bthd', weights, v).reshape(B, T, D)\n",
    "        return self.out_proj(out)\n",
    "attn = MultiHeadAttention(128, 128, 2)\n",
    "x=torch.randn(8, 10, 128)\n",
    "myattn = attn(x)\n",
    "\n",
    "y = x.permute(1, 0, 2)\n",
    "mha2_out = torch.nn.functional.multi_head_attention_forward(\n",
    "    query=y,\n",
    "    key=y,\n",
    "    value=y,\n",
    "    embed_dim_to_check=128,\n",
    "    num_heads=2,\n",
    "    use_separate_proj_weight=True,\n",
    "    in_proj_weight=None,\n",
    "    in_proj_bias=None,\n",
    "    bias_k=None,\n",
    "    bias_v=None,\n",
    "    need_weights=False,\n",
    "    q_proj_weight=attn.q_proj.weight,\n",
    "    k_proj_weight=attn.k_proj.weight,\n",
    "    v_proj_weight=attn.v_proj.weight,\n",
    "    out_proj_bias=attn.out_proj.bias,\n",
    "    out_proj_weight=attn.out_proj.weight,\n",
    "    add_zero_attn=False,\n",
    "    dropout_p=0,\n",
    ")[0].permute(1, 0, 2)\n",
    "print(myattn.shape)\n",
    "print(torch.allclose(mha2_out, myattn, atol=1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
