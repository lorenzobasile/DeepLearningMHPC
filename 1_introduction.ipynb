{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5C6vCuBhjjGm"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorenzobasile/DeepLearningMHPC/blob/main/1_introduction.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J-xj0WE5-Hp"
   },
   "source": [
    "\n",
    "# Lab 1: Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onJ4CutWhFjA"
   },
   "source": [
    "Lecturer: Lorenzo Basile (lorenzo.basile@areasciencepark.it), Research Fellow at AREA Science Park\n",
    "\n",
    "\n",
    "Every lab will finish with a small homework that builds on the day's material. We can briefly discuss any doubt or curiosity the following day. The homeworks have to be completed within the end of the week.\n",
    "\n",
    "Please, do reach out for any doubt and clarification about the course and/or the assignments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zieSJTheEHb"
   },
   "source": [
    "## Computational resources\n",
    "\n",
    "We will not run particularly heavy experiments during the labs, so for most parts you should be able to reproduce the experiments on the CPU of your personal laptop. However, to avoid issues with library versions and to avoid installing any package (and to take advantage of some hardware acceleration from time to time), we will be running the labs on Google Colab, a service that provides free access GPUs.\n",
    "\n",
    "For your assignments, it is advisable to switch from Colab to a proper HPC facility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD1NrDYig7Ev"
   },
   "source": [
    "# Introduction to Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxhBDiM7hwnZ"
   },
   "source": [
    "Colab is a free service provided by Google for ML research. It is based on Jupyter notebooks that run on a remote server, and it provides free (but limited time) GPU acceleration.\n",
    "\n",
    "To enable GPU or TPU acceleration just go to `Runtime>Change runtime type` and choose from the menu. Please note that GPU usage is limited in time, so avoid requesting one if you do not really need it.\n",
    "\n",
    "Inside a code cell you can use `!` to run shell commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yobTsVNBsNZo"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi    # if you enable GPU acceleration, this command returns information on the GPU\n",
    "!pip install torch==1.11.0    # just an example, torch is already installed in Colab\n",
    "!sudo apt-get install gcc    # you can also run sudo commands\n",
    "!wget https://roboti.us/download/mujoco200_linux.zip    # and download data to a temporary memory\n",
    "!git clone git@github.com:lorenzobasile/DeepLearningMHPC.git    # just another example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efPBqlRFkWaO"
   },
   "source": [
    "## Colab file system\n",
    "\n",
    "By default, Colab accesses a volatile memory that is erased as soon as your process terminates, but you can interface it with your personal Google Drive to read and write data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD70La0jmV9S"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKgtLsgU3Eaq"
   },
   "outputs": [],
   "source": [
    "# note that if you want this command to be permanent you need to use the magic % instead of !\n",
    "%cd drive/MyDrive/DeepLearningMHPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZdJ_J8ymbVm"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDmeXyfm8Sjj"
   },
   "source": [
    "# Getting started with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJIabHGPAyjA"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7RMvAS7_FZu"
   },
   "source": [
    "## What is PyTorch?\n",
    "\n",
    "PyTorch (or informally torch) is a Python library specifically built for Deep Learning, that comes with a series of very useful functionalities that make it one of the most popular tools for DL research and application.\n",
    "\n",
    "Namely, it has many built-in features and modules useful for DL, tensor arithmetic and automatic differentiation features, and it allows for easy GPU acceleration through CUDA.\n",
    "\n",
    "Another famous library for DL you may have heard of is TensorFlow, which also has a more user-friendly interface called Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Whh-6LIWBaeO"
   },
   "source": [
    "## Basic operation with Tensors\n",
    "\n",
    "The main building block of PyTorch is the `Tensor` class. A torch `Tensor` is the equivalent of NumPy `ndarray` and most of the functionalities are the same as in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpD4ME148gl3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=torch.tensor([[1,2,3],[4,5,6]])\n",
    "y=np.array([[1,2,3],[4,5,6]])\n",
    "\n",
    "print(\"X:\", x)\n",
    "print(\"Y:\", y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qtg8opJNFAUL"
   },
   "source": [
    "Basic NumPy array features exist for torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6znBc2hDtDh"
   },
   "outputs": [],
   "source": [
    "x.shape, y.shape, x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOBslFT5FCRz"
   },
   "outputs": [],
   "source": [
    "x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRewW0F_G7Q0"
   },
   "source": [
    "Note that you can build a tensor through the constructor `torch.Tensor`. In this case, since `torch.Tensor` is an alias for `torch.FloatTensor`, the tensor you create will have type `torch.float32`.\n",
    "\n",
    "You can convert the dtype of a tensor by using the functions `float()`, `int()` etc.\n",
    "\n",
    "More info on data types [here](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6LBpF5FHSgD"
   },
   "outputs": [],
   "source": [
    "x=torch.Tensor([[1,2,3], [4,5,6]])\n",
    "print(\"Dtype of X:\", x.dtype)\n",
    "x=x.int()\n",
    "print(\"Dtype of X:\", x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tensors, such as those made by ones and zeros can be created using their corresponding functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.ones(2,3,2)\n",
    "print(\"Ones:\", x)\n",
    "x=torch.zeros(2,3,2)\n",
    "print(\"Zeros:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bqa2Dz8dGFnI"
   },
   "source": [
    "And you can create random tensors just like you create random arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UoOMPGuFMf2"
   },
   "outputs": [],
   "source": [
    "x=torch.rand(2,3,2)    # you can also use a list or a tuple for the dimensions\n",
    "y=np.random.rand(2,3,2)\n",
    "print(\"X:\", x)\n",
    "print(\"Y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgXgfVm7MctR"
   },
   "source": [
    "You can easily compute statistics of tensors (such as the sum, mean, max, min, std... of the elements) by either using the methods of the `Tensor` class or using the basic torch functions and using your tensor as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gjgFjspFT-3"
   },
   "outputs": [],
   "source": [
    "x.sum(), torch.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "px8JQCXlMuE-"
   },
   "outputs": [],
   "source": [
    "x.mean(), torch.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsIvuPJ1M9E0"
   },
   "outputs": [],
   "source": [
    "x.argmin(), torch.argmin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDlly60nNDuk"
   },
   "source": [
    "It is sometimes very useful to specify one or more dimensions to reduce (along which you want to perform your operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2taOpNVK5Fk"
   },
   "outputs": [],
   "source": [
    "print(x)\n",
    "x.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0yfA2jqMFo4"
   },
   "outputs": [],
   "source": [
    "x.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pArlxdjJNXs1"
   },
   "outputs": [],
   "source": [
    "x.sum(dim=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfL7l01dOZ89"
   },
   "source": [
    "Tensor slicing works exactly like in NumPy, by means of square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3g-ytVvbNtmH"
   },
   "outputs": [],
   "source": [
    "x[0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkBPG4UCOZJI"
   },
   "outputs": [],
   "source": [
    "x[0,1:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gCnGoHXPGM-"
   },
   "outputs": [],
   "source": [
    "x[:,::2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5,6)\n",
    "y = torch.randn_like(x)\n",
    "z = torch.ones_like(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two key functions to aggregate different tensors are `torch.cat` and `torch.stack`. \n",
    "\n",
    "In a nutshell, `cat` concatenates the tensors along a given dimension. All tensors must have the same shape, except at most for the concatenation dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.cat([x,y,z], dim=0)\n",
    "print(\"Concatenated shape:\", c.shape)\n",
    "\n",
    "new_c = torch.cat([c,y,z], dim=0)\n",
    "print(\"New concatenated shape:\", new_c.shape)\n",
    "\n",
    "new_c = torch.cat([c,y,z], dim=1) # this does not work\n",
    "print(\"New concatenated shape:\", new_c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stack` instead creates a new dimension, along which the input tensors get aggregated. In this case, the shapes need to match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.stack([x,y,z], dim=0)\n",
    "print(\"Stack shape:\", c.shape)\n",
    "\n",
    "new_c = torch.cat([c,y,z], dim=0) # this does not work\n",
    "print(\"New stack shape:\", new_c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2Zn_AKJPsYf"
   },
   "source": [
    "## Linear algebra and tensor reshaping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6LQprgNQmGx"
   },
   "source": [
    "An operation we will frequently perform in Deep Learning (though often under the hood) is matrix multiplication. In torch, it can be done in many equivalent ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDXKKSCvPueK"
   },
   "outputs": [],
   "source": [
    "x=torch.rand(4,5)\n",
    "y=x.T    # matrix transposition\n",
    "\n",
    "print(x@y)\n",
    "print(x.matmul(y))\n",
    "print(torch.matmul(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUPJEi4LRC6F"
   },
   "source": [
    "Please note that the operator for matrix multiplication is `@`, not `*`, which indicates the Hadamard (element-wise) product instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EduEPVkcPw9M"
   },
   "outputs": [],
   "source": [
    "x*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p8e9LpSRgqv"
   },
   "source": [
    "Multiplying a matrix by itself is obviously equivalent to computing its power, and it can be done also by running one of the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6wkhv7NRaLl"
   },
   "outputs": [],
   "source": [
    "torch.pow(x,2), x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwqdFzYhSf8v"
   },
   "source": [
    "As in NumPy, there exists a `dot` function to compute the scalar product between vectors. Note that differently from NumPy, in torch this is **not** equivalent to matrix multiplication, as it is intended to work only with 1D vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nB7ZAzO4RnCn"
   },
   "outputs": [],
   "source": [
    "v1=x[:,1]\n",
    "v2=x[:,2]\n",
    "print(v1.shape, v2.shape)\n",
    "\n",
    "print(v1.dot(v2))    # in the case of 1D vectors, there is no difference between row and column vectors\n",
    "print(v1.matmul(v2))\n",
    "print(v1@v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yryQXyVoTVUJ"
   },
   "source": [
    "If you want to do something fancier with two vectors, like multiplying a column by a row to obtain a matrix, you need to switch to 2D vectors by reshaping them or use `torch.outer`.\n",
    "\n",
    "When you reshape a tensor, you can leave one dimension unspecified (using -1), as it can be inferred automatically by torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1HHk1CYR5Kk"
   },
   "outputs": [],
   "source": [
    "v1_reshaped=v1.reshape(-1,1)    # column vector\n",
    "v2_reshaped=v2.reshape(1,-1)    # row vector\n",
    "\n",
    "print(v1_reshaped.shape, v2_reshaped.shape)\n",
    "result1 = v1_reshaped@v2_reshaped\n",
    "result2 = torch.outer(v1,v2)\n",
    "print(torch.allclose(result1, result2, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o76-vuJYSEqI"
   },
   "outputs": [],
   "source": [
    "print(v1_reshaped.dot(v2_reshaped))    # this doesn't work! dot works only on 1D tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH-zNQceYDMq"
   },
   "source": [
    "Changing the shape of a tensor is a crucial operation in DL. To have an idea of its application, just think of RGB images, commonly used in Computer Vision.\n",
    "\n",
    "These are $3\\times H\\times W$ tensors, where H and W stand for height and width of the image (in number of pixels). It is often needed to regard an image as a linearized (flattened array of pixels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1mkOCIIUhMM"
   },
   "outputs": [],
   "source": [
    "img=torch.stack([torch.zeros(8,8), torch.ones(8,8), torch.zeros(8,8)], dim=0)\n",
    "img.reshape(3,64)    # note that reshaping is not in place, so this call does not change the actual shape of img\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHH3StilnKWc"
   },
   "source": [
    "Very often (for instance when you have to pass an image to `matplotlib` for visualization), you need to change the shape of an image to $H\\times W \\times 3$. You may be tempted to do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhzYnjRrnhuB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "new_img=img.reshape(8,8,3)\n",
    "\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4tEQZnjnoMn"
   },
   "source": [
    "This piece of code runs seamlessly, since the dimensions are consistent with the original ones. However, it will not produce the expected behaviour (a green image).\n",
    "\n",
    "In fact, `reshape` only modifies the shape of a tensor, without touching the way data are stored in memory, meaning that you would end up mixing data from different dimensions.\n",
    "\n",
    "The right way to change the order of dimensions is to use `permute`, which accepts as argument the ordering of dimensions that you desire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rQYWvChJ3BD"
   },
   "outputs": [],
   "source": [
    "new_img=img.permute(1,2,0)\n",
    "print(new_img.shape)\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBdhQQc0ckAA"
   },
   "source": [
    "# Building Machine Learning models with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDp7I1jqc1ZT"
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "By using all the pieces we've seen till now, we can build our first ML model using PyTorch: a linear regressor, whose model is:\n",
    "\n",
    "$$\n",
    "y = XW + b\n",
    "$$\n",
    "\n",
    "which can also be simplified as:\n",
    "\n",
    "$$\n",
    "y = XW\n",
    "$$\n",
    "\n",
    "if we incorporate the bias $b$ inside $W$ and add to the $X$ a column of ones to the right.\n",
    "\n",
    "\n",
    "We start by creating our data. We randomly sample $X$ as a $N\\times P$ tensor, meaning that we have 1000 datapoints and 100 features and compute $y$ as:\n",
    "$$\n",
    "y=XM+\\mathcal{N}(0,I)\n",
    "$$\n",
    "where $M$ is a randomly drawn projection vector (shape $P\\times 1$, same as our weights).\n",
    "We are adding some iid gaussian noise on the $y$ to avoid the interpolation regime, in which we could be fitting our data perfectly using a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rih2SvjkcyGk"
   },
   "outputs": [],
   "source": [
    "N=1000\n",
    "P=100\n",
    "X=torch.rand(N,P)\n",
    "M=torch.rand(P,1)\n",
    "Y=X@M+torch.normal(torch.zeros(N,1),torch.ones(N,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzqfSPVANPsK"
   },
   "source": [
    "We can add a column of ones to $X$ to include the bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inmRPdHDNWNE"
   },
   "outputs": [],
   "source": [
    "X=torch.cat([X, torch.ones(N,1)], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhSx42OFK6YK"
   },
   "source": [
    "The regression can be fit with classical statistical methods such as Ordinary Least Squares, and the optimal $W$ has the form:\n",
    "\n",
    "$$\n",
    "W^*=(X^TX)^{-1}X^Ty\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjHK46Taf6EP"
   },
   "outputs": [],
   "source": [
    "W_star = ((X.T @ X).inverse()) @ X.T @ Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_star.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WONzQvFMw5d"
   },
   "source": [
    "To assess the quality of this fit we can evaluate the Mean Squared Error (MSE) between the original $y$ and the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jJG9dxIgTY8"
   },
   "outputs": [],
   "source": [
    "torch.nn.functional.mse_loss(X@W_star, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXdU0Hn0R387"
   },
   "source": [
    "## The same linear model, but in PyTorch style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8T5AHAXPaR3"
   },
   "source": [
    "A linear model like the one we saw before is nothing more than an artificial neuron with no activation function.\n",
    "\n",
    "We will now be exploring the second chunk of PT functionalities, namely the built-in structures and routines supporting the creation of ML models.\n",
    "\n",
    "We can create the same model we have seen before using torch built-in structures, so we start to see them right away.\n",
    "\n",
    "Usually, a torch model is a `class` inheriting from `torch.nn.Module`. Inside this class, we'll define two methods:\n",
    "* the constructor (`__init__`) in which we define the building blocks of our model as class variables;\n",
    "* the `forward` method, which specifies how the data fed into the model needs to be processed in order to produce the output.\n",
    "\n",
    "Note for those who already know something about NNs: we don't need to define `backward` methods since we're constructing our model with built-in PT building blocks. PyTorch automatically creates a `backward` routine based upon the `forward` method.\n",
    "\n",
    "Our model only has one building block (layer) which is a `Linear` layer.\n",
    "We need to specify the size of the input (i.e. the coefficients $W$ of our linear regressor) and the size of the output (i.e. how many scalars it produces) of the layer. We additionaly request our layer to have a bias term $b$.\n",
    "\n",
    "The `Linear` layer processes its input as $XW + b$, which is exactly the (first) equation of the linear regressor we saw before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wo-mlrrdPfYo"
   },
   "outputs": [],
   "source": [
    "class LinearRegressor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.regressor = torch.nn.Linear(in_features=P, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.regressor(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4BsHWq8QpCM"
   },
   "source": [
    "We can create an instance of our model and inspect the current parameters by using the `state_dict` method, which prints the building blocks of our model and their current parameters. Note that `state_dict` is essentially a dictonary indexed by the names of the building blocks which we defined inside the constructor (plus some additional identifiers if a layer has more than one set of parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac6Y3DpDQaxr"
   },
   "outputs": [],
   "source": [
    "model = LinearRegressor()\n",
    "\n",
    "for param_name, param in model.state_dict().items():\n",
    "    print(param_name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMfs7q6KQ-kT"
   },
   "source": [
    "We can update the parameters via `state_dict` and re-using the same OLS estimates we obtained before.\n",
    "\n",
    "Note that torch is thought of for Deep Learning: it does not have the routines to solve different ML problems (just use `sklearn` for this).\n",
    "\n",
    "Next time, we'll see how we can unleash gradient-based iterative training routines in torch and compare the results w.r.t. the OLS estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5DbBSqhQvSF"
   },
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "state_dict[\"regressor.weight\"] = W_star[:P].T\n",
    "state_dict[\"regressor.bias\"] = W_star[P]\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEcg3K2HRSba"
   },
   "outputs": [],
   "source": [
    "X_lin_reg = X[:,:P]\n",
    "predictions_lin_reg = model(X_lin_reg) # equivalent to lin_reg.foward(X_lin_reg)\n",
    "ols_loss = torch.nn.functional.mse_loss(predictions_lin_reg, Y)\n",
    "print(ols_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation in torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PyTorch is built with support for differentiation in mind.\n",
    "In the end, Deep Learning (for now) is all about differentiation and building cascades of differentiable function into complicated multilayer deep neural networks.\n",
    "\n",
    "Essentially, all PyTorch built-ins support differentiability (unless the function is not differentiable, of course).\n",
    "Today we will see how to compute derivatives in PyTorch.\n",
    "\n",
    "\n",
    "Under the hood, each torch `Tensor` has a boolean attribute `requires_grad`, which tells `autograd` whether it should keep trace of operations on the tensor or not. `autograd` is the automatic differentiation engine of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,3)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually set this to `True` or create directly a Tensor supporting grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True\n",
    "# or equivalently x=torch.rand(3, 3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we are handling a function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$.\n",
    "\n",
    "For instance, $f(x) = x^2$.\n",
    "\n",
    "We could apply $f$ to a singleton tensor and compute the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, requires_grad=True)\n",
    "\n",
    "print(\"x:\", x)\n",
    "\n",
    "y = x**2\n",
    "\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the gradient, we call `backward()` on the Tensor `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the derivative to be $2x$. We can check this is correct by inspecting the gradient of `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad==2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when there's no gradient, `grad` is automatically set to `None` to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(3,3).grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same operation can be repeated in a slightly more complicated setting, when dealing with a scalar function of more than one variable $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$\n",
    "\n",
    "This step is particularly important since the core operation of Deep Learning is computing the gradients of a scalar function (our loss function) with respect to the parameters of the network.\n",
    "\n",
    "Now `x` will be a vector (or a matrix, it doesn't really matter for our case) and we will apply to it a function which returns a single scalar.\n",
    "\n",
    "One example may be $f(\\mathbf{x})=\\sum_{i=1}^d x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([5], requires_grad=True)\n",
    "\n",
    "print(x)\n",
    "\n",
    "y = x.sum()\n",
    "\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composition of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use also `backward` to compute the gradient of a composition of functions. For our objective, it will be very useful to think in terms of computational graph.\n",
    "\n",
    "We can view $y=g(f(x))$ as\n",
    "\n",
    "![](images/compgra1.jpg)\n",
    "\n",
    "We might extend this and add a hidden node $z$\n",
    "between $f$ and $g$\n",
    "\n",
    "![](images/compgra2.jpg)\n",
    "\n",
    "Supposing $z=f(x)=\\log(x)$\n",
    "and $y=g(z)=z^2$, we can reproduce this example in PyTorch. \n",
    "\n",
    "To sum up, $y=\\log^2(x)$, and so we expect $\\frac{dy}{dx}=2\\frac{\\log(x)}{|x|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, requires_grad=True)\n",
    "\n",
    "print(\"x:\", x)\n",
    "\n",
    "z = x.log()\n",
    "\n",
    "y = z**2\n",
    "\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z.retain_grad()\n",
    "\n",
    "y.backward()\n",
    "\n",
    "x.grad==2*x.log()/x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we want to access $\\frac{dy}{dz}=2z$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad==2*z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store gradients of intermediate computations, we can call `.retain_grad()` on the intermediate node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see another feature of torch differentiation functionalities.\n",
    "\n",
    "We can call `backward()` multiple times; let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "x_2 = torch.tensor([2.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = x_1.cos() * x_2.log()\n",
    "c.backward()\n",
    "print(x_1.grad, x_2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening? Why the gradient is not the same?\n",
    "\n",
    "PyTorch continues to accumulate (i.e., sum) the gradients. If we want to reset the gradient, we must set it to None\n",
    "```\n",
    "x_1.grad = None\n",
    "x_2.grad = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tools to compute gradients of any function (specifically, we will be interested in loss functions), it is time to figure out what to do with these gradients.\n",
    "\n",
    "In torch, it is straightforward to use most optimization techniques based on Gradient Descent and its variations, such as SGD, Adam, RMSProp etc.\n",
    "\n",
    "To do so, you should exploit the tools of the `torch.optim` package, that contains the most famous training algorithms in the `Optimizer` class. To cite some:\n",
    "\n",
    "*   `torch.optim.SGD`\n",
    "*   `torch.optim.Adam`\n",
    "*   `torch.optim.RMSprop`\n",
    "*   `torch.optim.Adagrad`\n",
    "\n",
    "To construct an Optimizer you have to give it an iterable containing the parameters to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the lr parameter is mandatory, there is no default\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an optimizer is very simple, there are three main steps:\n",
    "\n",
    "*   delete the current gradient information on the parameters: `optimizer.zero_grad()`\n",
    "*   compute the derivatives of the loss: `loss.backward()`\n",
    "*   perform a gradient descent step and update parameters (according to the algorithm in use): `optimizer.step()`\n",
    "\n",
    "Note that the first step should always be performed since, as we saw, torch accumulates gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At times, it is useful to vary the learning rate while training, for example you may want to use a large learning rate in the initial phase of training to quickly descend the loss and then you may want to decrease it to be more precise around a minimum. To do so, the easiest way is to use schedulers that you can find in `torch.optim.lr_scheduler`.\n",
    "\n",
    "The simplest LR scheduler is `ExponentialLR`, which takes a parameter $\\gamma$ and at each step does:\n",
    "\n",
    "$$\n",
    "\\text{lr}=\\gamma \\text{lr}\n",
    "$$\n",
    "\n",
    "More info on this [here](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate), but in a nutshell all you have to do is something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "#and then, when you want to actually decrement the learning rate\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to our linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to the linear regression from the previous lab, we now have all the tools to fit its weights and bias with Stochastic Gradient Descent (SGD) instead of using Least Squares.\n",
    "\n",
    "First of all, let's reset our regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg=LinearRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2V-wW_hqqtl"
   },
   "source": [
    "### Definition of a loss function\n",
    "\n",
    "One key element that we need to train any neural network is a loss function, i.e. a function that quantifies how *good* is our fit to the data and that is differentiable w.r.t. the weights and biases of the network.\n",
    "\n",
    "We saw some examples of common loss functions in the lecture, and all the main losses used in Deep Learning are already implemented and available in PyTorch, to cite some:\n",
    "\n",
    "*   `torch.nn.MSELoss`\n",
    "*   `torch.nn.CrossEntropyLoss`\n",
    "*   `torch.nn.BCELoss`\n",
    "*   `nn.KLDivLoss`\n",
    "\n",
    "You can also define your own custom loss function, and as long as you use built-in torch functions to compute it (and you keep it differentiable), you should be fine.\n",
    "\n",
    "For example, you could build your own MSE loss like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEUjpIJBqpzH"
   },
   "outputs": [],
   "source": [
    "def mseloss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17Kjge2LFXJu"
   },
   "source": [
    "### Definition of a DataLoader object\n",
    "\n",
    "To train any PyTorch model, it is useful to handle data through a `DataLoader` object. A `DataLoader` is an iterable wrapped around a `Dataset` object that allows to easily run through your data in batches.\n",
    "\n",
    "For any specific need, you can build your own `Dataset` class. To make it work properly, you always have to implement three functions: `__init__`, `__len__` and `__getitem__`. More info on this [here](https://pytorch.org/docs/stable/data.html).\n",
    "\n",
    "Starting from a set of `Tensor`s representing features and labels, it is easy to define the `Dataset` and its corresponding `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf982KSbFUpu"
   },
   "outputs": [],
   "source": [
    "dataset=torch.utils.data.TensorDataset(X[:,:-1],Y) # the last column of X is the bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `batch_size` of 8. This means that at each training step the network is fed with 8 datapoints from our dataset, on which a gradient descent step is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJnoNnObIDso"
   },
   "outputs": [],
   "source": [
    "dataloader=torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rfKtE_yIIax"
   },
   "outputs": [],
   "source": [
    "X_0, y_0=next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KniM65qIZw3"
   },
   "outputs": [],
   "source": [
    "print(X_0, y_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing regression, so our objective is the minimization of the Mean Squared Error between the targets and the output of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go on and define our optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(lin_reg.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the nested loop that we will run to train the network. Its pseudocode is as follows:\n",
    "\n",
    "\n",
    "```\n",
    "Loop over epochs:\n",
    "    Loop over data:\n",
    "        Perform a forward pass\n",
    "        Compute the loss\n",
    "        Erase the past gradients\n",
    "        Compute gradients performing a backward pass\n",
    "        Update the parameters\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "SGD training is an iterative process, that we repeat for a (usually) fixed number of *epochs*. In each epoch we traverse the whole dataset by exploiting our `DataLoader` object, that provides us with randomly drawn mini-batches of 8 elements.\n",
    "\n",
    "We can keep track of the loss so that we can compare it with the loss of the least squares estimate we obtained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "losses=[]\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=0 # we want to accumulate the loss on all the mini-batches to average and obtain the overall MSE\n",
    "    for x, y in iter(dataloader):\n",
    "        out=lin_reg(x)\n",
    "        l=loss(out, y)\n",
    "        epoch_loss+=l.item()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(epoch_loss/len(dataloader)) # len(dataloader) contains the number of mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss rapidly decreases during training, reaching the one obtained with least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Linear regession loss\")\n",
    "plt.axhline(ols_loss.detach(), color='r', label='Least Squares')\n",
    "plt.semilogy(losses, label='SGD')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Optimization epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to solve our first *real* problem with Deep Learning. The task is handwritten digit recognition, and we will use the MNIST dataset. It is an outstandingly popular benchmark in the Machine Learning community, and it is seen as the first and simplest real-world task one can solve with neural network (aka the *Hello World of Deep Learning*).\n",
    "\n",
    "First of all, let's download the data and create our DataLoaders.\n",
    "\n",
    "The transforms we are employing are intended to convert the images into torch Tensors (`ToTensor()`) and to normalize the images to have mean 0 and standard deviation 1 (`Normalize`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST('./data/', transform=transforms,  train=True, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST('./data/', transform=transforms, train=False, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can visualize our data using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(iter(trainloader))\n",
    "print(x.shape)\n",
    "first_img=x[0]\n",
    "first_label=y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For grayscale images, `imshow` expects input of shape $H\\times W$, so we have to reshape the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(first_img.reshape(28,28), cmap='gray')\n",
    "print(\"Label: \", first_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go on and define our model: a MLP with two hidden layers of 32 and 16 neurons each, with ReLU activations. The input layer size is 784, since our images are 28x28, and the output layer has 10 neurons since we have 10 classes.\n",
    "\n",
    "ReLU is the Rectified Linear Unit, defined as:\n",
    "$$\n",
    "\\text{ReLU}(x)=\\cases{0\\hspace{0.5cm}\\text{if } x\\lt 0\\\\ x\\hspace{0.5cm}\\text{if } x\\ge 0}\n",
    "$$\n",
    "\n",
    "Note that the `Linear` layer of torch expects the input to be a vector, so in the `forward` method we have to remember to flatten the image into a 1-D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=28*28, out_features=32, bias=True)\n",
    "        self.layer2 = torch.nn.Linear(in_features=32, out_features=16, bias=True)\n",
    "        self.layer3 = torch.nn.Linear(in_features=16, out_features=10, bias=True)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        # alternatively, self.activation = torch.nn.functional.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.reshape(-1,28*28)\n",
    "        x=self.activation(self.layer1(x))\n",
    "        x=self.activation(self.layer2(x))\n",
    "        x=self.layer3(x) # we don't need any nonlinearity (i.e. softmax) on the output layer. why?\n",
    "        return x\n",
    "\n",
    "model=Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our optimizer and the loss function that we want to minimize. Since we are doing 10-class classification we now switch to Cross-Entropy.\n",
    "\n",
    "You can find the details of this loss in torch [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), but what is really important to point out is that it is assumed that the input to this loss are \"raw, unnormalized scores for each class\". This means that we shouldn't include any softmax in the network architecture, since it is already included by this implementation of the CE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "loss=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a utility function to compute how accurate is our classifier.\n",
    "\n",
    "Note the `model.eval()` command and the context manager `with torch.no_grad()`.The former is telling the network that we are evaluating it, not training it: in this simple example there is no real use for this clarification, but we will see cases when this is crucial; the latter is switching off gradient operations: this allows you to make computations lighter and to avoid performing gradient descent steps by mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct=0\n",
    "        dataset_loss=[]\n",
    "        for x, y in iter(dataloader):\n",
    "            out=model(x)\n",
    "            dataset_loss.append(loss(out, y).item())\n",
    "            correct+=(torch.argmax(out, dim=1)==y).sum()\n",
    "            #correct+=(out.argmax(-1)==y).sum()\n",
    "        val_loss=sum(dataset_loss)/len(dataloader)\n",
    "        return correct/len(dataloader.dataset), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, we have to pay attention to specifying `model.train()`, the converse of `model.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5\n",
    "losses=[]\n",
    "val_losses=[]\n",
    "for epoch in range(epochs):\n",
    "    acc, val_loss = get_accuracy(model, testloader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"Test accuracy: \", acc)\n",
    "    model.train()\n",
    "    print(\"Epoch: \", epoch)\n",
    "    batch_losses=[]\n",
    "    for x, y in iter(trainloader):\n",
    "        out=model(x)\n",
    "        l=loss(out, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        batch_losses.append(l.item())\n",
    "    losses.append(sum(batch_losses)/len(batch_losses))\n",
    "print(\"Final accuracy: \", get_accuracy(model, testloader))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"MNIST batch loss\")\n",
    "plt.plot(losses, label='training')\n",
    "plt.plot(val_losses, label='test')\n",
    "plt.xlabel(\"Optimization step\")\n",
    "plt.ylabel(\"CE Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 1. Read carefully the paper [Learning representations by back-propagating errors](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)\n",
    "- 2. Reproduce in PyTorch (or any other DL library you like) experiment 1. Try to be as close as possible to the original protocol regarding network architecture, activation function, training algorithm and parameter initialization.\n",
    "    - Inspect the weights you obtained and check if they provide a solution to the problem\n",
    "    - Compare the solution to the solution reported in the paper\n",
    "    - Be careful: don't expect to be able to fully reproduce the results of the paper."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
