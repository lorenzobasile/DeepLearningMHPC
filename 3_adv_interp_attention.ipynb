{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inkmEpop3Rb7"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorenzobasile/DeepLearningMHPC/blob/main/3_adv_interp_attention.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3YBQpy13Rb8"
      },
      "source": [
        "# Lab 3: Adversarial attacks, Interpretability and Attention mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtNUON-5Bn84"
      },
      "source": [
        "### Recap from previous Lab\n",
        "\n",
        "* We saw the main techniques to mitigate overfitting in neural networks;\n",
        "* We built and trained a convolutional network for image classification;\n",
        "* We saw how to leverage pre-trained parameters to transfer the network's knowledge to new tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpCQCoySBn84"
      },
      "source": [
        "### Today\n",
        "\n",
        "We will show an example of one of the most intriguing phenomena in neural networks, **adversarial vulnerability**, in the context of CIFAR-10 classification. Then, we will interpret the behaviour of our image classifier using **GradCAM**. Finally, we will move away from CNNs and towards transformers, by implementing an **attention** layer from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hll3sjlJBn84"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92ielIOfBn85"
      },
      "source": [
        "# Adversarial attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4HAW0LVBn85"
      },
      "source": [
        "Adversarial attacks are small and maliciously crafted perturbations to the input data of DL systems, usually imperceptible to human eyes but highly disruptive for the neural network. By exploiting the modelâ€™s sensitivity to slight changes in input, these attacks can cause incorrect and often unpredictable behavior, even in well-trained systems.\n",
        "\n",
        "<img src=\"https://github.com/lorenzobasile/DeepLearningMHPC/blob/main/images/adv.png?raw=1\" width=\"800\"/>\n",
        "\n",
        "This phenomenon reveals fundamental weaknesses in how neural networks process information, highlighting a gap between human perception and machine learning models. Understanding adversarial attacks is crucial for assessing the reliability of deep learning, especially in applications where robustness and security are critical, such as autonomous driving or medical diagnosis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZu9siK0Bn85"
      },
      "source": [
        "## Training a CNN on CIFAR-10\n",
        "\n",
        "The first step we take is training a convolutional classifier on CIFAR-10 images. CIFAR-10 is another popular benchmark dataset, much like MNIST. However, it represents a significant step up in terms of complexity from MNIST: images are now coloured, and slightly larger (each image is represented as 32x32 pixels over 3 (red, green, blue) channels). They belong to 10 categories: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
        "\n",
        "We are not aiming for state-of-the-art performance, but to improve the classification capability of our CNN, we can use light data augmentation. When using `torchvision` datasets, it is very simple to include data augmentation by exploiting the `transforms` submodule. Each time an image is loaded from the dataloader, it may get horizontally flipped and/or cropped, adding to the variability of the data distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KsoqvZ7Bn86"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "transforms = {\n",
        "    'train': torchvision.transforms.Compose([\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        torchvision.transforms.RandomCrop(32, 4),\n",
        "        torchvision.transforms.ToTensor()\n",
        "        ]),\n",
        "    'test': torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ]),\n",
        "}\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms['train'])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms['test'])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-Khc_KDBn86"
      },
      "source": [
        "The data and network architecture we will be using today start to approach real-world sizes. For this experiment, it is a good idea to speed-up training through GPU acceleration. GPU runtime can be chosen in Colab by simply using the menu option `Runtime>Change runtime type`. Once this has been done, the following PyTorch command will automatically perceive `cuda:0` (the first GPU) as the current device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDehQjloBn86"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yeLE1guS6lt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNzwrq5QBn86"
      },
      "source": [
        "Our network has a simple structure, with 2 convolutional layers and ReLU activations. We have to pay extra attention to the device: by default, PyTorch loads the model (and everything else) to CPU; if a GPU has to be used, the model must be loaded to the correct device by using `.to(device)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-fYs0MNBn86"
      },
      "outputs": [],
      "source": [
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, stride=1, padding=0),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.pool = torch.nn.Sequential(\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AvgPool2d(kernel_size=2),\n",
        "            torch.nn.Flatten()\n",
        "        )\n",
        "\n",
        "        self.head = torch.nn.Linear(128*7*7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        self.last_feature_map = x\n",
        "        # this will become useful later\n",
        "        if self.last_feature_map.requires_grad:\n",
        "            self.last_feature_map.retain_grad()\n",
        "        x = self.pool(x)\n",
        "        return self.head(x)\n",
        "\n",
        "model = CNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeq7fkiBn86"
      },
      "source": [
        "Just the usual optimizer and loss definition..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej83OqbnBn86"
      },
      "outputs": [],
      "source": [
        "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss=torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrOjzEVlBn86"
      },
      "source": [
        "...and the usual code to compute the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geL-bZjbBn86"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct=0\n",
        "        for x, y in iter(dataloader):\n",
        "            x=x.to(device)\n",
        "            y=y.to(device)\n",
        "            out=model(x)\n",
        "            correct+=(torch.argmax(out, axis=1)==y).sum()\n",
        "        return (correct/len(dataloader.dataset)).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbjWhxgABn86"
      },
      "source": [
        "Our model reaches a competitive accuracy relatively quickly. This value is far from the state-of-the-art on CIFAR-10 (which is well above 99%), but it just serves as a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyu-IX6UBn86"
      },
      "outputs": [],
      "source": [
        "epochs=10\n",
        "for epoch in range(epochs):\n",
        "    print(\"Test accuracy: \", get_accuracy(model, testloader, device))\n",
        "    model.train()\n",
        "    print(\"Epoch: \", epoch)\n",
        "    for x, y in iter(trainloader):\n",
        "        x=x.to(device)\n",
        "        y=y.to(device)\n",
        "        out=model(x)\n",
        "        l=loss(out, y)\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "print(\"Final accuracy: \", get_accuracy(model, testloader, device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y36pgVQlBn86"
      },
      "source": [
        "## Fast Gradient Sign attack\n",
        "\n",
        "We will implement the simplest form of gradient-based adversarial attack, the Fast Gradient Sign Method (FGSM), introduced by Goodfellow et al. in 2014. Given an input image $x$, the adversarial perturbation is simply computed as the gradient of the loss function (in our case, the cross-entropy) with respect to $x$:\n",
        "$$\n",
        "\\Delta = \\text{sign}(\\nabla_x L(y,\\hat{y}))\n",
        "$$\n",
        "The perturbation is then rescaled by a factor $\\epsilon$, so that it does not exceed a given threshold in $\\ell_\\infty$ norm. Common values for $\\epsilon$ include $\\{1,2,3,...,8\\}/255$.\n",
        "\n",
        "The adversarial image is then obtained as:\n",
        "$$\n",
        "x'=x+\\epsilon\\Delta\n",
        "$$\n",
        "After this computation, we clamp the perturbed image in $[0,1]$ as this was the original range for the pixels of the clean image.\n",
        "\n",
        "In this lab, given its simplicity, we are implementing FGSM from scratch. However, in standard research practise, attack algorithms can be taken directly from libraries, such as [advertorch](https://advertorch.readthedocs.io/en/latest/index.html) or [foolbox](https://foolbox.readthedocs.io/en/stable/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUx4lu0RBn87"
      },
      "outputs": [],
      "source": [
        "def fgsm_attack(model, image, label, epsilon=8/255):\n",
        "    # ensure the image requires gradients for the attack\n",
        "    image.requires_grad = True\n",
        "\n",
        "    output = model(image)\n",
        "    loss = torch.nn.functional.cross_entropy(output, label)\n",
        "\n",
        "    # backward pass: compute gradients of the loss w.r.t. the image\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # create adversarial example\n",
        "    perturbed_image = image + epsilon * image.grad.sign()\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)  # keep pixel values valid\n",
        "\n",
        "    return perturbed_image, perturbed_image-image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu0YwPR1Bn87"
      },
      "source": [
        "We have to compute gradients to apply FGSM, hence the context manager `torch.no_grad()` cannot be applied here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsRiGuTmBn87"
      },
      "outputs": [],
      "source": [
        "def get_adversarial_accuracy(model, dataloader, attack, device):\n",
        "    model.eval()\n",
        "    correct=0\n",
        "    for x, y in iter(dataloader):\n",
        "        x=x.to(device)\n",
        "        y=y.to(device)\n",
        "        adv, _ =attack(model, x, y)\n",
        "        with torch.no_grad():\n",
        "            out=model(adv)\n",
        "            correct+=(torch.argmax(out, axis=1)==y).sum()\n",
        "    return (correct/len(dataloader.dataset)).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Lf_DyHBn87"
      },
      "source": [
        "A very small perturbation, $\\epsilon=\\frac{4}{255}$ is already enough to disrupt most of the classifier's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF26RASfBn87"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "get_adversarial_accuracy(model, testloader, partial(fgsm_attack, epsilon=8/255), device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG49meSeBn87"
      },
      "source": [
        "We can now visualize a few examples of clean and adversarially attacked images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gplzw57iBn87"
      },
      "outputs": [],
      "source": [
        "x,y=next(iter(testloader))\n",
        "x=x.to(device)\n",
        "y=y.to(device)\n",
        "adv, pert =fgsm_attack(model, x, y, 8/255)\n",
        "adversarial_y = model(adv).argmax(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ-R5HyzBn87"
      },
      "outputs": [],
      "source": [
        "image_idx=1\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
        "\n",
        "axes[0].imshow(x[image_idx].detach().cpu().permute(1, 2, 0))\n",
        "axes[0].set_title(f\"Clean: {testset.classes[y[image_idx]]}\")\n",
        "\n",
        "axes[1].imshow(adv[image_idx].detach().cpu().permute(1, 2, 0))\n",
        "axes[1].set_title(f\"Adversarial: {testset.classes[adversarial_y[image_idx]]}\")\n",
        "\n",
        "axes[2].imshow(pert[image_idx].detach().cpu().permute(1, 2, 0))\n",
        "axes[2].set_title(f\"Delta: {testset.classes[adversarial_y[image_idx]]}\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pert[1].max()"
      ],
      "metadata": {
        "id": "6uSVTviVAuQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdtk9k0YBn87"
      },
      "source": [
        "## Adversarial defense\n",
        "\n",
        "Defending against adversarial attacks is complex, and it is still an open problem in DL research.\n",
        "\n",
        "A possible approach is to apply adversarial training. This method foresees a double optimization: the network gets fine-tuned using both clean and adversarial data. Note: adversarial data depends on current network weights; when doing adversarial training, the attack algorithm has to be run again after each optimization step.\n",
        "\n",
        "Adversarial training can significantly improve robustness, at the cost of a (hopefully small) drop in clean accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArsdEuruBn87"
      },
      "outputs": [],
      "source": [
        "epochs=10\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss=torch.nn.CrossEntropyLoss()\n",
        "for epoch in range(epochs):\n",
        "    print(\"Initial test accuracy: \", get_accuracy(model, testloader, device))\n",
        "    print(\"Initial adversarial accuracy: \", get_adversarial_accuracy(model, testloader,partial(fgsm_attack, epsilon=4/255), device))\n",
        "    model.train()\n",
        "    print(\"Epoch: \", epoch)\n",
        "    for x, y in iter(trainloader):\n",
        "        x=x.to(device)\n",
        "        y=y.to(device)\n",
        "        clean_out=model(x)\n",
        "        l=loss(clean_out, y)\n",
        "        adv, pert =fgsm_attack(model, x, y, 4/255)\n",
        "        adv_out=model(adv)\n",
        "        l+=loss(adv_out, y)\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "print(\"Final clean accuracy: \", get_accuracy(model, testloader, device))\n",
        "print(\"Final adversarial accuracy: \", get_adversarial_accuracy(model, testloader,partial(fgsm_attack, epsilon=4/255), device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qnb2ARxPOkQ"
      },
      "source": [
        "# GradCAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic8gkDKJZCHC"
      },
      "source": [
        "Neural networks, especially deep ones, are often criticized as black boxes: they can achieve impressive accuracy, but give us little insight into why they make a particular decision. This lack of interpretability can a serious concern in applications where understanding the modelâ€™s reasoning is crucial.\n",
        "\n",
        "GradCAM (Gradient-weighted Class Activation Mapping, Selvaraju et al., 2017) is a simple but powerful method to visualize what parts of an input image a convolutional neural network focuses on when making a prediction. It works by tracing back the gradients from a specific class score to the last convolutional layer (or, in principle, any other convolutional layer), producing a heatmap that highlights the regions of the image most influential to the decision. This helps us see whether the network is attending to the right features or relying on spurious patterns.\n",
        "\n",
        "<img src=\"https://github.com/lorenzobasile/DeepLearningMHPC/blob/main/images/gradcam.png?raw=1\" width=\"800\"/>\n",
        "\n",
        "GradCAM is straightforward to implement, assuming that one has access to the activations and gradients of the convolutional layer of interest.\n",
        "\n",
        "In a nutshell, starting from the pre-softmax activation of the desired class $y_c$, the GradCAM activation map can be computed as:\n",
        "$$\n",
        "M_c(i,j)=\\text{ReLU}(\\sum_k \\alpha_k^c A_k(i,j))\n",
        "$$\n",
        "where $\\alpha_k^c$ is obtained as:\n",
        "$$\n",
        "\\alpha_k^c=\\frac{1}{Z}\\sum_{i,j}\\frac{\\partial{y_c}}{\\partial A_k(i,j)}\n",
        "$$\n",
        "\n",
        "The heatmap computed by GradCAM has the same spatial dimension as the convolutional filters, which usually differs from the input dimension. To 'project' the heatmap in the input space by using bilinear interpolation.\n",
        "\n",
        "We will be using a lot the `squeeze` and `unsqueeze` functions of torch. `squeeze` removes all the dimensions of size 1 from the tensor; `unsqueeze` adds a dimension of size 1 where specified.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3,4)"
      ],
      "metadata": {
        "id": "sE3RsCR3BbRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=x.unsqueeze(dim=0)\n",
        "print(x.shape)\n",
        "x=x.unsqueeze(dim=-1)\n",
        "print(x.shape)\n",
        "x=x.squeeze()\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "NM3FieUBBfyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrrIZxkIPRiX"
      },
      "outputs": [],
      "source": [
        "def gradcam(image, label, model):\n",
        "    model.eval()\n",
        "    image=image.unsqueeze(0) # we need the batch dimension for the forward pass\n",
        "    output=model(image).squeeze() # but we can remove it afterwards\n",
        "    prediction = output.argmax(-1)\n",
        "    print(\"Prediction: \", testset.classes[prediction])\n",
        "    print(\"True label: \", testset.classes[label])\n",
        "    model.zero_grad()\n",
        "    output[label].backward() # we are doing backprop not on the loss function, but on the output\n",
        "    feature_maps = model.last_feature_map.squeeze() # we remove the batch dimension\n",
        "    weights = model.last_feature_map.grad.squeeze().mean(dim=(1,2)) # weight is the average importance over a filter\n",
        "    weighted_feature_maps=(feature_maps*weights.reshape(-1,1,1)) # broadcasting\n",
        "    grad_cam=torch.nn.functional.relu(weighted_feature_maps.sum(dim=0))\n",
        "    grad_cam = grad_cam.unsqueeze(0).unsqueeze(0) # interpolate wants [N,C,H,W] data\n",
        "    print(grad_cam.shape)\n",
        "    grad_cam = torch.nn.functional.interpolate(grad_cam, size=(image.shape[2:]), mode='bilinear', align_corners=False)\n",
        "    return grad_cam.squeeze().detach().cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04tmpLitBFUw"
      },
      "source": [
        "Once the class activation map has been computed, it can be simply overlaid to the original image to see what parts of it matter the most in making the classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHQ-ydspXFiK"
      },
      "outputs": [],
      "source": [
        "x,y=next(iter(testloader))\n",
        "x=x.to(device)\n",
        "y=y.to(device)\n",
        "image_idx=0\n",
        "\n",
        "heatmap = gradcam(x[image_idx], y[image_idx], model)\n",
        "heatmap = heatmap - heatmap.min()\n",
        "heatmap = heatmap / heatmap.max()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "axes[0].imshow(x[image_idx].detach().cpu().permute(1, 2, 0))\n",
        "axes[0].set_title(f\"Image: {testset.classes[y[image_idx]]}\")\n",
        "\n",
        "axes[1].imshow(x[image_idx].detach().cpu().permute(1, 2, 0))\n",
        "axes[1].set_title(f\"GradCAM: {testset.classes[y[image_idx]]}\")\n",
        "axes[1].imshow(heatmap, alpha=heatmap, cmap='jet', interpolation='bilinear')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gJVVlu6Bn87"
      },
      "source": [
        "# The attention mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpYIUr3qBFUw"
      },
      "source": [
        "Attention is the core computational mechanism of transformer models. The aim of attention is to make different *tokens* communicate, either within the same sequence (self-attention) or between different sequences (cross-attention). In this notebook, we will implement self-attention, as it is the most common in modern transformers.\n",
        "\n",
        "Given an input sequence of tokens $X$, the computation of attention starts by projecting it into three corresponding sequences $Q$ (queries), $K$ (keys) and $V$ (values), using learnable projection matrices:\n",
        "\n",
        "$$\n",
        "Q=W_QX, \\quad K=W_KX, \\quad V=W_VX\n",
        "$$\n",
        "\n",
        "Then, an *attention map* is produced. This map, whose shape is $|X|\\times|X|$, encodes the relevance of each key entry with respect to each query entry. In simple terms, it represents how important is a position in the sequence to another.\n",
        "\n",
        "$$\n",
        "A = \\text{Softmax}(\\frac{QK^T}{d_K})\n",
        "$$\n",
        "\n",
        "Once these weights are computed, the output sequence is obtained by weighting the values and applying a final projection $W_O$:\n",
        "\n",
        "$$\n",
        "Y = W_O(A V)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y96UkGMuBFUw"
      },
      "source": [
        "## Self-attention\n",
        "\n",
        "As a first step, we will implement the self-attention computation in plain PyTorch, by only using linear layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhlII1_uBn87"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.d = output_dim\n",
        "        self.q_proj = torch.nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.k_proj = torch.nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.v_proj = torch.nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.out_proj = torch.nn.Linear(output_dim, output_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "        unnormalized_map = torch.einsum('btHd,bTHd->bHtT', q, k)/math.sqrt(self.d)\n",
        "        weights = torch.nn.functional.softmax(unnormalized_map, dim=-1)\n",
        "        out = torch.einsum('BtT,BTd->Btd', weights, v)\n",
        "        #alternative with bmm\n",
        "        #unnormalized_map = torch.bmm(q, k.permute(0,2,1))/math.sqrt(self.d)\n",
        "        #out = torch.bmm(weights, v)\n",
        "        return self.out_proj(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF6QEtcCBFUz"
      },
      "source": [
        "Actually, torch implements attention, so there is no need to write this code from scratch when using transformers. We can compare our implementation with the built-in of torch to verify that there are no difference in the results.\n",
        "\n",
        "In most applications, the simplest way to use attention is by means of the class [MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html), but here we use the functional interface because it allows greater flexibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY0nwdAtBFUz"
      },
      "outputs": [],
      "source": [
        "attn = Attention(128, 128)\n",
        "x=torch.randn(8, 10, 128)\n",
        "myattn = attn(x)\n",
        "\n",
        "#in torch, the multihead attention is implemented supposing the first dimension is the token dimension\n",
        "y = x.permute(1, 0, 2)\n",
        "\n",
        "mha2_out = torch.nn.functional.multi_head_attention_forward(\n",
        "    query=y,\n",
        "    key=y,\n",
        "    value=y,\n",
        "    embed_dim_to_check=128,\n",
        "    num_heads=1,\n",
        "    use_separate_proj_weight=True,\n",
        "    in_proj_weight=None,\n",
        "    in_proj_bias=None,\n",
        "    bias_k=None,\n",
        "    bias_v=None,\n",
        "    need_weights=False,\n",
        "    q_proj_weight=attn.q_proj.weight,\n",
        "    k_proj_weight=attn.k_proj.weight,\n",
        "    v_proj_weight=attn.v_proj.weight,\n",
        "    out_proj_bias=attn.out_proj.bias,\n",
        "    out_proj_weight=attn.out_proj.weight,\n",
        "    add_zero_attn=False,\n",
        "    dropout_p=0,\n",
        ")[0].permute(1, 0, 2)\n",
        "print(torch.allclose(mha2_out, myattn, atol=1e-6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCt-uI6bBn87"
      },
      "source": [
        "# Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOrzI22IBn87"
      },
      "source": [
        "- Read the paper [Adversarial Examples in the Physical World](https://arxiv.org/pdf/1607.02533), and implement the Basic Iterative Method explained in section 2.2. In short, BIM is an iterative version of FGSM.\n",
        "- Test your code on our CNN, classifying CIFAR-10 data, and visually inspect the parts of the image that the attack is using to misguide the network. For this step, you can use GradCAM on some clean and adversarial images, using the correct label in the former case and the adversarial one in the latter.\n",
        "- Perform adversarial training on BIM, and keep track of clean and adversarial accuracy during training. Extra: you can also measure the robustness of the model trained on BIM on the vanilla FGSM attack."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4O4A2ykE107"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}